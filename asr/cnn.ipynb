{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8496c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(source_dir, dest_dir, split=(0.8, 0.1, 0.1), seed=42):\n",
    "    random.seed(seed)\n",
    "    labels = ['cm', 'non_cm']\n",
    "    \n",
    "    for label in labels:\n",
    "        imgs = list(Path(source_dir, label).glob(\"*.png\"))\n",
    "        random.shuffle(imgs)\n",
    "        \n",
    "        n = len(imgs)\n",
    "        n_train = int(split[0] * n)\n",
    "        n_val = int(split[1] * n)\n",
    "        \n",
    "        splits = {\n",
    "            'train': imgs[:n_train],\n",
    "            'val': imgs[n_train:n_train+n_val],\n",
    "            'test': imgs[n_train+n_val:]\n",
    "        }\n",
    "\n",
    "        for split_name, files in splits.items():\n",
    "            split_dir = Path(dest_dir, split_name, label)\n",
    "            split_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for img in files:\n",
    "                shutil.copy(img, split_dir / img.name)\n",
    "\n",
    "# Example usage\n",
    "split_data(\"spectrograms\", \"split_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1752349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # resize if needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(\"split_data/train\", transform=transform)\n",
    "val_data = datasets.ImageFolder(\"split_data/val\", transform=transform)\n",
    "test_data = datasets.ImageFolder(\"split_data/test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy library for numerical operations\n",
    "import numpy as np\n",
    "# Import torch library for building and training neural networks\n",
    "import torch\n",
    "# Import nn module from torch for building neural network layers\n",
    "from torch import nn\n",
    "# Import torch multiprocessing module for parallel processing\n",
    "import torch.multiprocessing\n",
    "# Import SummaryWriter module from torch.utils.tensorboard for logging to TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Import summary function from torchsummary for displaying model summary\n",
    "from torchsummary import summary\n",
    "# Import torchvision library for image processing\n",
    "import torchvision\n",
    "# Import pyplot module from matplotlib for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "# Import tqdm module for displaying progress bars\n",
    "from tqdm.auto import tqdm\n",
    "# Import default_timer function from timeit for measuring time taken for model training\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "writer_path = 'runs/log_file_tensorboard'\n",
    "# writer to log to tensorboard\n",
    "writer = SummaryWriter(writer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caccfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_WORKERS = 4 # number of worker used when loading data into dataloader\n",
    "# DATASET_PATH = 'split_data' # path of our spectrogram dataset\n",
    "IMAGE_SIZE = (128, 128) # image size\n",
    "CHANNEL_COUNT = 3 # 3 channel as an image has 3 color (R,G,B)\n",
    "ATTRIBUTION = [\"cm\", \"non_cm\"] # class labels\n",
    "ACCURACY_THRESHOLD = 90 # accuracy at which to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network class that inherits from PyTorch nn.Module.\n",
    "class neuralNetworkV1(nn.Module):\n",
    "    # The __init__ method is used to declare the layers that will be used in the forward pass.\n",
    "    def __init__(self):\n",
    "        super().__init__() # required because our class inherit from nn.Module\n",
    "        # First convolutional layer with 3 input channels for RGB images, 16 outputs (filters).\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        # Second convolutional layer with 16 input channels to capture features from the previous layer, 16 outputs (filters).\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        # Third and fourth convolutional layers with 16 and 10 output channels respectively.\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=2, padding=1)\n",
    "        # Max pooling layer to reduce feature complexity.\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # ReLU activation function for introducing non-linearity.\n",
    "        self.relu = nn.ReLU()\n",
    "        # Flatten the 2D output from the convolutional layers for the fully connected layer.\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layer connecting to 1D neurons, with 3 output features for 3 classes.\n",
    "        self.linear = nn.Linear(in_features=480, out_features=3)\n",
    "    \n",
    "    # define how each data sample will propagate in each layer of the network\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pooling(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pooling(x)\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.flatten(x)\n",
    "        try:\n",
    "            x = self.linear(x)\n",
    "        except Exception as e:\n",
    "            print(f\"Error : Linear block should take support shape of {x.shape} for in_features.\")\n",
    "        return x\n",
    "\n",
    "our_model = neuralNetworkV1()\n",
    "\n",
    "print(\"Model summary : \")\n",
    "print(summary(our_model, (CHANNEL_COUNT, IMAGE_SIZE[0], IMAGE_SIZE[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00335d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display total time training\n",
    "def display_training_time(start, end):\n",
    "    total_time = end - start\n",
    "    print(f\"Training time : {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "# Display training infos for each epochs\n",
    "def display_training_infos(epoch, val_loss, train_loss, accuracy):\n",
    "    val_loss = round(val_loss.item(), 2)\n",
    "    train_loss = round(train_loss.item(), 2)\n",
    "    accuracy = round(accuracy, 2)\n",
    "    print(f\"Epoch : {epoch}, Training loss : {train_loss}, Validation loss : {val_loss}, Accuracy : {accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f907d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates accuracy between truth labels and predictions.\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "# The core function for training the CNN\n",
    "def train_neural_net(epochs, model, loss_func, optimizer, train_batches, val_batches):\n",
    "    final_accuracy = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # training mode\n",
    "        model.train()\n",
    "        with torch.enable_grad():\n",
    "            train_loss = 0\n",
    "            for images, labels in train_batches:\n",
    "                predictions = model(images)\n",
    "                loss = loss_func(predictions, labels)\n",
    "                train_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss /= len(train_batches)\n",
    "            writer.add_scalar(\"training loss\", train_loss, epoch)\n",
    "        # evaluation mode\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for images, labels in val_batches:\n",
    "                predictions = model(images)\n",
    "                val_loss += loss_func(predictions, labels)\n",
    "                val_accuracy += accuracy_fn(y_true=labels, y_pred=predictions.argmax(dim=1))\n",
    "            val_loss /= len(val_batches)\n",
    "            val_accuracy /= len(val_batches)\n",
    "            writer.add_scalar(\"validation loss\", val_loss, epoch)\n",
    "            final_accuracy = val_accuracy\n",
    "        display_training_infos(epoch+1, val_loss, train_loss, val_accuracy)\n",
    "        writer.add_scalar(\"accuracy\", val_accuracy, epoch)\n",
    "        if val_accuracy >= ACCURACY_THRESHOLD:\n",
    "            break\n",
    "    return final_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
